# 1 Transformer 结构

首先是我们非常熟悉的Transfomer架构图，你可能已经看到过非常多版本的这类图片。最基本的结构就是`Encoder-Decoder`结构。相关代码存放在`others/transformer`目录。

<div align=center>
<img src="https://ask.qcloudimg.com/http-save/yehe-1490568/9b6fd5d3aef61b05ebea633c351bd8bc.png" align="middle" width="70%">
</div>
<div align=center>图1 Transfomer 示意图</div>

```python
class Transformer(nn.Module):
    def __init__(self, inputs_vocab, outputs_vocab, d_model, N, heads, dropout):
        super(Transformer, self).__init__()
        self.encoder = Encoder(inputs_vocab, d_model, N, heads, dropout)
        self.decoder = Decoder(outputs_vocab, d_model, N, heads, dropout)
        self.outproject = nn.Linear(d_model, outputs_vocab)
    
    def forward(self, inputs, shift_outputs, inputs_mask, outputs_mask):
        e_outputs = self.encoder(inputs, inputs_mask)
        d_output = self.decoder(shift_outputs, e_outputs,inputs_mask, outputs_mask)
        output = self.outproject(d_output)
        return output
```

这样就是一个基本的结构啦！我们有解码器，也有编码器，同时还有两个输入。这里先额外说明以下`Transformer`的推理过程，便于你更好的理解。

我们通过一个“机器翻译”的来举例，假设我们要将"I love you!"翻译成中文, 正确回答我们定义为“我爱你！”。那我们的`inputs`就是`I love you!`, 此时`shift_outputs`为空，预测的结果`outputs`为`我`;在下一阶段，我们的`inputs`就是`I love you!`, 此时`shift_outputs`为`我`，预测的结果`outputs`为`爱`......这样循环往复，就可以得到最终的预测结果`我爱你！`。


注, 为什么会存在mask变量:
1. **处理非定长序列（Padding Mask）**：在自然语言处理（`NLP`）任务中，文本通常是不定长的。为了将不同长度的文本输入到模型中，需要对较短的文本进行`padding`（填充），以形成一个固定大小的张量输入。在这种情况下，`mask`用于指示哪些位置是填充的，哪些是实际的文本数据。这样，模型就可以忽略填充的部分，只关注实际的文本数据。

2. **防止标签泄露（Sequence Mask）**：在序列生成任务中，如语言模型或机器翻译，为了防止模型“看到”未来的信息，需要使用`mask`来遮盖未来的时间步。这在`Transformer`的解码器中尤为重要，因为解码器需要基于已经生成的序列来预测下一个序列，而不应该依赖于未来的序列信息。通过使用`mask`，可以确保模型在预测当前`token`时，不会受到未来`token`的影响。

# 2 Token Embedding

让我们从输入开始看，输入`inputs`首先是经过了一个名为`Token Embedding`的处理。我们可以这样理解这个过程：输入是一段句子，但机器是无法理解句子的，所以需要通过`Token Embedding`进行初步处理，转换成机器能识别的内容。

机器只能够识别具体的数字。所以我们第一步要做的是将句子切分成一个个的`word`，通过处理之后，使得`word`可以用一个一维向量来表示，类似于这这种:
- "I": [1,2,3,4,5,6]
- "love": [2,3,4,5,6,1]
- "you": [3,4,5,6,2,1]
- "!": [4,5,6,8,8,1]

当然，现在展示给你的是我自己瞎写的，具体应该表示为多少维度的内容具体为什么的向量，这个需要让模型自己学习得到。

我们人脑看到一个`word`, 比如`love`，脑海中浮现出来的可能是罗密欧朱丽叶之流，然后进而理解到这个意思，而机器则是联想到一个向量。这样做有很多好处，比如，人可以轻而易举的认为`love`和`like`是有相似之处的，而机器则可以通过计算`love`和`like`的向量表示的余弦相似度来得出这两个词有共同之处，即余弦相似度越大，`word`之间的含义越相似。（不同的`LLM`可能采用不同的计算方法，不一定要是余弦相似度。）

这个地方我们直接使用`torch`自带的库去实现就好，如果对这部分原理非常感兴趣，可以从NLP最初的`BOW`,`TF-IDF`和`word2vec`等方法开始了解科学家们是如何开始用向量表示`word`的。

# 3 位置编码层

这部分的作用的目的就是为了让`word`的表示向量中，体现出`顺序`的概念，我们很容易就理解到顺序是非常重要的。


<div align=center>
<img src="https://mengbaoliang.cn/wp-content/uploads/2023/02/9193d17bcedf292.png" align="middle" width="100%">
</div>
<div align=center>图2 位置编码层公式</div>


图2中的`d`表示的是向量的维度，通过这样一种编码方式，我们就可以给向量赋予位置信息。而且位置编码的后续处理也非常简单。只需将位置编码的向量，与通过`Token Embedding`的向量相加即可。

代码实现：
```python
class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_len):
        super(PositionalEncoder, self).__init__()
        self.d_model = d_model
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos][i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))
                pe[pos][i + 1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))
                
        pe = pe.unsqueeze(0)
        self.register_buffer('pe',pe)

    def forward(self, x):
        x = self.embedding(x)
        x = x * math.sqrt(self.d_model)

        seq_len = x.size(1)
        x = (x + torch.tensor(self.pe[:,:seq_len], requires_grad=False)).cuda()
        return x

```

# 4 Encoder

现在我们需要考虑`Encoder`的写法。 首先输入经过嵌入层和位置编码层，然后经过多个编码器，处理后，归一化输出。

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super(Encoder, self).__init__()
        self.N = N
        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pos = PositionalEncoder(d_model)
        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)
        self.norm = NormLayer(d_model)
        
    def forward(self, inputs, mask):
        x = self.embed(inputs)
        x = self.pos(x)
        for i in range(self.N):
            x = self.layers[i](x, mask)
        return self.norm(x)
```

编码层写法:

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.attn = MultiHeadAttention(heads, d_model, dropout)
        self.ff = FeedForward(d_model, dropout)
        self.norm = NormLayer(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask):
        x = x + self.attn(x, x, x, mask)
        x = self.norm(x)
        x = x + self.ff(x)
        x = self.norm(x)
        return x
```

经过多头注意力层，归一化，然后进入前馈神经网络，然后再次归一化，这就是一个编码曾的基本实现。

## 4.1 多头注意力机制

多头注意力机制通过将注意力计算分配到多个头中，每个头学习不同的表示子空间，然后将这些表示合并起来，这部分的理解我建议看这个[视频](https://www.bilibili.com/video/BV14m421u7EM/?spm_id_from=333.337.search-card.all.click&vd_source=a7a5ff2f9d1f5a8f5e83d4a2d2ed8fb0)。

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        self.d_k = d_model // heads
        self.h = heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)

    def attention(q, k, v, d_k, mask=None, dropout=None):
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # 上下文单词所对应的权重得分，形状是 seq_len, d_model × d_model, seq_len = seq_len, seq_len
        # 掩盖掉那些为了填补长度增加的单元，使其通过 softmax 计算后为 0
        if mask is not None:
            mask = mask.unsqueeze(1)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        scores = F.softmax(scores, dim=-1)
        if dropout is not None:
            scores = dropout(scores)
        output = torch.matmul(scores, v)
        return output
    
    def forward(self, q, k, v, mask=None):
        bs = q.size(0)
        
        # 进行线性操作划分为 h 个头， batch_size, seq_len, d_model -> batch_size, seq_len, h, d_k
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)  
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)

        # 矩阵转置  batch_size, seq_len, h, d_k -> batch_size, h, seq_len, d_k
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)

        # 计算 attention
        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)
        
        # 连接多个头并输入到最后的线性层 (bs, h, seq_len, d_k) 转换为 (bs, seq_len, h, d_k)
        # .contiguous() 用于确保内存的连续性，方便后续的操作。
        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)
        output = self.out(concat)
        return output

```

## 4.2 前馈神经网络

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, dropout=0.1):
        super().__init__(FeedForward, self)
        d_ff = d_model * 4
        
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        x = self.dropout(F.relu(self.linear1(x)))
        x = self.linear2(x)
```

# 5 Decoder


到这里，其实解码器也就是和上述中编码器相差不大的实现方法了，可以具体内容可以参照代码：

```python
class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super(DecoderLayer, self).__init__()
        self.norm_1 = NormLayer(d_model)
        self.norm_2 = NormLayer(d_model)
        self.norm_3 = NormLayer(d_model)
        
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)

        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.ff = FeedForward(d_model, dropout=dropout)

    def forward(self, x, e_outputs, src_mask, trg_mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))  # q是x2，k是encoder的输出，v也是encoder的输出
        x2 = self.norm_3(x)
        x = x + self.dropout_3(self.ff(x2))
        return x


class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super(Decoder, self).__init__()
        self.N = N
        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)
        self.norm = NormLayer(d_model)
    
    def forward(self, trg, e_outputs, src_mask, trg_mask):
        x = self.embed(trg)   # 用的是trg
        x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, e_outputs, src_mask, trg_mask)
        return self.norm(x)

```

# 6 参考内容

[1] [Transformer代码实现](https://github.com/HuYinghua-Ethan/Transformer-)\
[2] [tiktok面试题：手写Transformer](https://www.bilibili.com/video/BV144421S7Wg/?spm_id_from=333.337.search-card.all.click&vd_source=a7a5ff2f9d1f5a8f5e83d4a2d2ed8fb0)\
[3] [Transformer模型详解，Attention is all you need](https://www.bilibili.com/video/BV14m421u7EM/?spm_id_from=333.337.search-card.all.click&vd_source=a7a5ff2f9d1f5a8f5e83d4a2d2ed8fb0)